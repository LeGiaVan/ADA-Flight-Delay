import pandas as pd
from pathlib import Path
import sys

def analyze_parquet_uniques(parquet_dir, columns_to_check):
    """
    ƒê·ªçc t·ª´ng file Parquet v√† t·ªïng h·ª£p c√°c gi√° tr·ªã duy nh·∫•t (Unique Values).
    Gi√∫p debug l·ªói schema v√† ƒë·ªëi chi·∫øu d·ªØ li·ªáu.
    """
    print(f"üîç B·∫ÆT ƒê·∫¶U PH√ÇN T√çCH: {parquet_dir}")
    print(f"üéØ C√°c c·ªôt c·∫ßn ki·ªÉm tra: {columns_to_check}\n")

    # T√¨m t·∫•t c·∫£ file parquet
    files = list(Path(parquet_dir).rglob("*.parquet"))
    if not files:
        print("‚ùå Kh√¥ng t√¨m th·∫•y file Parquet n√†o!")
        return

    # Dictionary ƒë·ªÉ l∆∞u th·ªëng k√™: { 't√™n_c·ªôt': { 'gi√°_tr·ªã': s·ªë_l·∫ßn_xu·∫•t_hi·ªán } }
    master_counts = {col: {} for col in columns_to_check}
    
    # Dictionary ƒë·ªÉ l∆∞u ki·ªÉu d·ªØ li·ªáu: { 't√™n_c·ªôt': set(c√°c_ki·ªÉu_d·ªØ_li·ªáu_ƒë√£_g·∫∑p) }
    type_tracker = {col: set() for col in columns_to_check}

    total_rows = 0
    files_read = 0

    print(f"üìÇ T√¨m th·∫•y {len(files)} file. ƒêang x·ª≠ l√Ω...")

    for idx, file_path in enumerate(files):
        try:
            # Ch·ªâ ƒë·ªçc c√°c c·ªôt c·∫ßn thi·∫øt ƒë·ªÉ nhanh
            # L∆∞u √Ω: N·∫øu c·ªôt kh√¥ng t·ªìn t·∫°i trong file, pandas s·∫Ω b√°o l·ªói, ta c·∫ßn try-except
            try:
                df = pd.read_parquet(file_path, columns=columns_to_check)
            except Exception:
                # Fallback: N·∫øu file thi·∫øu c·ªôt n√†o ƒë√≥, ƒë·ªçc h·∫øt r·ªìi l·ªçc sau (ch·∫≠m h∆°n ch√∫t)
                df = pd.read_parquet(file_path)
                missing_cols = [c for c in columns_to_check if c not in df.columns]
                if missing_cols:
                    # B·ªè qua file n√†y ho·∫∑c warning n·∫øu c·∫ßn
                    continue
                df = df[columns_to_check]

            rows_in_file = len(df)
            total_rows += rows_in_file
            files_read += 1

            for col in columns_to_check:
                # 1. Check ki·ªÉu d·ªØ li·ªáu (ƒë·ªÉ debug l·ªói int vs dictionary)
                dtype = str(df[col].dtype)
                type_tracker[col].add(dtype)

                # 2. ƒê·∫øm value counts trong file n√†y
                v_counts = df[col].value_counts().to_dict()
                
                # 3. C·ªông d·ªìn v√†o master_counts
                for val, count in v_counts.items():
                    # Chuy·ªÉn val v·ªÅ string ƒë·ªÉ tr√°nh l·ªói hash key kh√°c ki·ªÉu
                    val_key = str(val) 
                    if val_key in master_counts[col]:
                        master_counts[col][val_key] += count
                    else:
                        master_counts[col][val_key] = count

            # In ti·∫øn ƒë·ªô
            if (idx + 1) % 10 == 0:
                print(f"   ... ƒê√£ ƒë·ªçc {idx + 1}/{len(files)} file ({total_rows:,} d√≤ng)")

        except Exception as e:
            print(f"‚ö†Ô∏è L·ªói ƒë·ªçc file {file_path.name}: {e}")

    print("\n" + "="*60)
    print(f"‚úÖ HO√ÄN TH√ÄNH! T·ªïng s·ªë d√≤ng ƒë√£ qu√©t: {total_rows:,}")
    print("="*60)

    # --- HI·ªÇN TH·ªä K·∫æT QU·∫¢ ---
    for col in columns_to_check:
        print(f"\nüìä PH√ÇN T√çCH C·ªòT: [{col}]")
        print(f"   - C√°c ki·ªÉu d·ªØ li·ªáu ƒë√£ g·∫∑p: {type_tracker[col]}")
        
        counts = master_counts[col]
        unique_count = len(counts)
        print(f"   - S·ªë l∆∞·ª£ng gi√° tr·ªã duy nh·∫•t (Cardinality): {unique_count}")
        
        # S·∫Øp x·∫øp theo s·ªë l∆∞·ª£ng gi·∫£m d·∫ßn
        sorted_counts = sorted(counts.items(), key=lambda item: item[1], reverse=True)
        
        print(f"   - Top 10 gi√° tr·ªã xu·∫•t hi·ªán nhi·ªÅu nh·∫•t:")
        print(f"     {'Gi√° tr·ªã':<20} | {'S·ªë l∆∞·ª£ng':<15} | {'T·ª∑ l·ªá %'}")
        print(f"     {'-'*20} | {'-'*15} | {'-'*10}")
        
        for val, count in sorted_counts[:15]: # Show top 15
            percent = (count / total_rows) * 100 if total_rows > 0 else 0
            print(f"     {str(val):<20} | {count:<15,} | {percent:.2f}%")
            
        if unique_count > 15:
            print(f"     ... v√† {unique_count - 15} gi√° tr·ªã kh√°c.")

# --- CH·∫†Y CODE ---
if __name__ == "__main__":
    # ƒê·ªïi ƒë∆∞·ªùng d·∫´n t·ªõi th∆∞ m·ª•c Parquet g·ªëc c·ªßa b·∫°n
    PARQUET_DIR = "D:/UEL/ADA/ADA-Flight-Delay/data/parquet/flights_weather/year=2023"
    
    # Danh s√°ch c·ªôt b·∫°n mu·ªën ki·ªÉm tra ƒë·ªëi chi·∫øu
    # Th√™m 'month' v√†o ƒë√¢y ƒë·ªÉ xem l·ªói g√¨
    COLS_TO_CHECK = ['OP_CARRIER', 'ORIGIN', 'MONTH'] 
    
    # N·∫øu file g·ªëc kh√¥ng c√≥ c·ªôt 'MONTH' (do partition), b·∫°n n√™n check c·ªôt kh√°c ho·∫∑c 'FL_DATE'
    # COLS_TO_CHECK = ['OP_CARRIER', 'ORIGIN'] 
    
    analyze_parquet_uniques(PARQUET_DIR, COLS_TO_CHECK)