import pandas as pd
import pyarrow.parquet as pq
import os
from pathlib import Path
import numpy as np
from datetime import datetime
import random


def analyze_dataset_characteristics(parquet_dir):
    """
    Ph√¢n t√≠ch ƒë·∫∑c ƒëi·ªÉm c·ªßa dataset ƒë·ªÉ x√°c ƒë·ªãnh:
    - H√£ng bay l·ªõn
    - S√¢n bay l·ªõn
    - Gi·ªù cao ƒëi·ªÉm
    """
    print("üîç ƒêang ph√¢n t√≠ch dataset ƒë·ªÉ x√°c ƒë·ªãnh ƒë·∫∑c ƒëi·ªÉm...")
    
    all_files = list(Path(parquet_dir).rglob("*.parquet"))
    print(f"   T√¨m th·∫•y {len(all_files):,} file Parquet")
    
    # L·∫•y sample nh·ªè ƒë·ªÉ ph√¢n t√≠ch (ƒë·ªçc 1 file m·ªói th√°ng)
    sample_files = []
    for month in range(1, 13):
        month_files = [f for f in all_files if f"month={month:02d}" in str(f)]
        if month_files:
            sample_files.append(month_files[0])
    
    print(f"   ƒêang ƒë·ªçc {len(sample_files)} file ƒë·ªÉ ph√¢n t√≠ch...")
    
    dfs = []
    for file in sample_files[:3]:  # Ch·ªâ ƒë·ªçc 3 file ƒë·∫ßu ƒë·ªÉ ph√¢n t√≠ch nhanh
        try:
            df = pd.read_parquet(file)
            dfs.append(df)
        except Exception as e:
            print(f"   L·ªói ƒë·ªçc {file}: {e}")
    
    if not dfs:
        print("‚ùå Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c file n√†o!")
        return None
    
    analysis_df = pd.concat(dfs, ignore_index=True)
    print(f"   ƒê√£ ƒë·ªçc {len(analysis_df):,} d√≤ng ƒë·ªÉ ph√¢n t√≠ch")
    
    # 1. X√°c ƒë·ªãnh h√£ng bay l·ªõn (d·ª±a tr√™n s·ªë chuy·∫øn)
    print("\nüìä Ph√¢n t√≠ch h√£ng bay...")
    carrier_counts = analysis_df['OP_CARRIER'].value_counts()
    top_carriers = carrier_counts.head(10).index.tolist()
    print(f"   Top 10 h√£ng bay: {top_carriers}")
    
    # 2. X√°c ƒë·ªãnh s√¢n bay l·ªõn (d·ª±a tr√™n s·ªë chuy·∫øn xu·∫•t ph√°t)
    print("\nüè¢ Ph√¢n t√≠ch s√¢n bay...")
    origin_counts = analysis_df['ORIGIN'].value_counts()
    dest_counts = analysis_df['DEST'].value_counts()
    
    # K·∫øt h·ª£p c·∫£ xu·∫•t ph√°t v√† ƒë·∫øn
    all_airports = pd.concat([origin_counts, dest_counts])
    airport_totals = all_airports.groupby(all_airports.index).sum()
    top_airports = airport_totals.sort_values(ascending=False).head(15).index.tolist()
    print(f"   Top 15 s√¢n bay: {top_airports[:5]}... (t·ªïng {len(top_airports)} s√¢n bay)")
    
    # 3. X√°c ƒë·ªãnh gi·ªù cao ƒëi·ªÉm
    print("\n‚è∞ Ph√¢n t√≠ch gi·ªù cao ƒëi·ªÉm...")
    if 'DEP_TIME' in analysis_df.columns:
        # Chuy·ªÉn DEP_TIME th√†nh gi·ªù
        analysis_df['DEP_HOUR'] = analysis_df['DEP_TIME'].astype(str).str[:2].fillna('00').astype(int)
        hour_counts = analysis_df['DEP_HOUR'].value_counts().sort_index()
        
        # Gi·ªù cao ƒëi·ªÉm: 6-9h s√°ng v√† 16-19h t·ªëi
        peak_hours = list(range(6, 10)) + list(range(16, 20))
        print(f"   Gi·ªù cao ƒëi·ªÉm x√°c ƒë·ªãnh: {peak_hours}")
    else:
        peak_hours = [7, 8, 9, 17, 18, 19]  # M·∫∑c ƒë·ªãnh
        print(f"   S·ª≠ d·ª•ng gi·ªù cao ƒëi·ªÉm m·∫∑c ƒë·ªãnh: {peak_hours}")
    
    return {
        'top_carriers': top_carriers,
        'top_airports': top_airports,
        'peak_hours': peak_hours,
        'total_files': len(all_files)
    }


def stratified_sampling_from_parquet(parquet_dir, output_csv, target_rows=6000, 
                                     sample_per_month=500, random_seed=42):
    """
    L·∫•y m·∫´u ph√¢n t·∫ßng t·ª´ c√°c file Parquet ƒë√£ ph√¢n v√πng
    
    Parameters:
    -----------
    parquet_dir : str
        Th∆∞ m·ª•c ch·ª©a c√°c file Parquet (c·∫•u tr√∫c year=2023/month=01/...)
    output_csv : str
        ƒê∆∞·ªùng d·∫´n file CSV output
    target_rows : int
        T·ªïng s·ªë d√≤ng m·ª•c ti√™u (~6000)
    sample_per_month : int
        S·ªë d√≤ng l·∫•y m·ªói th√°ng (~500)
    random_seed : int
        Seed cho random ƒë·ªÉ ƒë·∫£m b·∫£o reproducibility
    """
    
    np.random.seed(random_seed)
    random.seed(random_seed)
    
    print("=" * 70)
    print("üéØ B·∫ÆT ƒê·∫¶U STRATIFIED SAMPLING")
    print("=" * 70)
    
    # 1. Ph√¢n t√≠ch dataset
    characteristics = analyze_dataset_characteristics(parquet_dir)
    if not characteristics:
        print("‚ùå Kh√¥ng th·ªÉ ph√¢n t√≠ch dataset!")
        return
    
    top_carriers = characteristics['top_carriers']
    top_airports = characteristics['top_airports']
    peak_hours = characteristics['peak_hours']
    
    print(f"\nüìà ƒê·∫∑c ƒëi·ªÉm dataset:")
    print(f"   - H√£ng bay l·ªõn: {len(top_carriers)} h√£ng")
    print(f"   - S√¢n bay l·ªõn: {len(top_airports)} s√¢n bay")
    print(f"   - Gi·ªù cao ƒëi·ªÉm: {peak_hours}")
    
    # 2. T·∫°o th∆∞ m·ª•c output n·∫øu ch∆∞a t·ªìn t·∫°i
    output_path = Path(output_csv)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    all_sampled_data = []
    
    # 3. X·ª≠ l√Ω t·ª´ng th√°ng
    for month in range(1, 13):
        print(f"\n{'‚îÄ' * 50}")
        print(f"üìÖ X·ª≠ l√Ω th√°ng {month:02d}")
        print(f"{'‚îÄ' * 50}")
        
        month_dir = Path(parquet_dir) / f"year=2023" / f"month={month:02d}"
        
        if not month_dir.exists():
            print(f"   ‚ö†Ô∏è Th∆∞ m·ª•c {month_dir} kh√¥ng t·ªìn t·∫°i, b·ªè qua...")
            continue
        
        # ƒê·ªçc t·∫•t c·∫£ file Parquet trong th√°ng
        parquet_files = list(month_dir.glob("*.parquet"))
        print(f"   üìÅ T√¨m th·∫•y {len(parquet_files)} file Parquet")
        
        if not parquet_files:
            print(f"   ‚ö†Ô∏è Kh√¥ng c√≥ file Parquet trong th√°ng {month:02d}")
            continue
        
        # ƒê·ªçc v√† k·∫øt h·ª£p t·∫•t c·∫£ file trong th√°ng
        monthly_data = []
        for file_idx, parquet_file in enumerate(parquet_files):
            try:
                df = pd.read_parquet(parquet_file)
                monthly_data.append(df)
                
                if (file_idx + 1) % 5 == 0 or file_idx == len(parquet_files) - 1:
                    print(f"   üìñ ƒê√£ ƒë·ªçc {file_idx + 1}/{len(parquet_files)} file...")
                    
            except Exception as e:
                print(f"   ‚ùå L·ªói ƒë·ªçc {parquet_file.name}: {e}")
                continue
        
        if not monthly_data:
            print(f"   ‚ùå Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c file n√†o trong th√°ng {month:02d}")
            continue
        
        month_df = pd.concat(monthly_data, ignore_index=True)
        print(f"   üìä T·ªïng d√≤ng trong th√°ng {month:02d}: {len(month_df):,}")
        
        # 4. Ph√¢n lo·∫°i theo ƒë·ªô tr·ªÖ
        print(f"   üìä Ph√¢n lo·∫°i ƒë·ªô tr·ªÖ...")
        
        # Chu·∫©n b·ªã c√°c tr∆∞·ªùng c·∫ßn thi·∫øt
        if 'DEP_TIME' in month_df.columns:
            month_df['DEP_HOUR'] = month_df['DEP_TIME'].astype(str).str[:2].fillna('00').astype(int)
        else:
            month_df['DEP_HOUR'] = 0
        
        if 'ARR_DELAY' not in month_df.columns:
            print(f"   ‚ùå Kh√¥ng c√≥ c·ªôt ARR_DELAY trong d·ªØ li·ªáu")
            continue
        
        # Lo·∫°i b·ªè c√°c d√≤ng kh√¥ng c√≥ ARR_DELAY
        month_df_clean = month_df.dropna(subset=['ARR_DELAY'])
        print(f"   üßπ Sau khi l√†m s·∫°ch: {len(month_df_clean):,} d√≤ng")
        
        # Ph√¢n lo·∫°i
        severe_delay = month_df_clean[month_df_clean['ARR_DELAY'] > 60]
        moderate_delay = month_df_clean[(month_df_clean['ARR_DELAY'] > 30) & (month_df_clean['ARR_DELAY'] <= 120)]
        minor_delay = month_df_clean[(month_df_clean['ARR_DELAY'] > 0) & (month_df_clean['ARR_DELAY'] <= 30)]
        on_time = month_df_clean[month_df_clean['ARR_DELAY'] <= 0]
        
        print(f"   üìä Ph√¢n b·ªë ƒë·ªô tr·ªÖ:")
        print(f"     - Tr·ªÖ nhi·ªÅu (>60p): {len(severe_delay):,} d√≤ng")
        print(f"     - Tr·ªÖ v·ª´a (30-120p): {len(moderate_delay):,} d√≤ng")
        print(f"     - Tr·ªÖ √≠t (0-30p): {len(minor_delay):,} d√≤ng")
        print(f"     - ƒê√∫ng gi·ªù (‚â§0p): {len(on_time):,} d√≤ng")
        
        # 5. L·∫•y m·∫´u ph√¢n t·∫ßng cho th√°ng n√†y
        print(f"\n   üéØ L·∫•y m·∫´u ph√¢n t·∫ßng...")
        
        sampled_month = []
        
        # 5.1. L·∫•y m·∫´u TR·ªÑ NHI·ªÄU (50 d√≤ng)
        print(f"     üü• Tr·ªÖ nhi·ªÅu: ", end="")
        if len(severe_delay) > 0:
            # ∆Øu ti√™n: h√£ng l·ªõn, s√¢n bay l·ªõn, th·ªùi ti·∫øt x·∫•u
            severe_delay['score'] = (
                severe_delay['OP_CARRIER'].apply(lambda x: 2 if x in top_carriers else 0) +
                severe_delay['ORIGIN'].apply(lambda x: 1.5 if x in top_airports else 0) +
                severe_delay['DEST'].apply(lambda x: 1.5 if x in top_airports else 0) +
                severe_delay.apply(lambda row: 2 if (pd.notna(row.get('O_PRCP')) and row.get('O_PRCP', 0) > 0) else 0, axis=1) +
                severe_delay.apply(lambda row: 2 if (pd.notna(row.get('O_WSPD')) and row.get('O_WSPD', 0) > 20) else 0, axis=1)
            )
            
            # S·∫Øp x·∫øp theo ƒë·ªô tr·∫ª gi·∫£m d·∫ßn v√† score
            severe_delay_sorted = severe_delay.sort_values(
                by=['ARR_DELAY', 'score'], 
                ascending=[False, False]
            )
            
            # L·∫•y 50 d√≤ng, nh∆∞ng ƒë·∫£m b·∫£o c√≥ ƒë·ªß h√£ng
            n_severe = min(50, len(severe_delay_sorted))
            
            if n_severe > 0:
                # ƒê·∫£m b·∫£o m·ªói h√£ng l·ªõn c√≥ √≠t nh·∫•t 1-2 chuy·∫øn
                carriers_in_severe = severe_delay_sorted['OP_CARRIER'].unique()
                severe_samples = []
                
                for carrier in top_carriers[:5]:  # Top 5 h√£ng
                    carrier_flights = severe_delay_sorted[severe_delay_sorted['OP_CARRIER'] == carrier]
                    if len(carrier_flights) > 0:
                        n_carrier = min(3, len(carrier_flights))
                        severe_samples.append(carrier_flights.head(n_carrier))
                
                # Th√™m c√°c chuy·∫øn kh√°c ƒë·ªÉ ƒë·ªß s·ªë l∆∞·ª£ng
                remaining = n_severe - sum(len(df) for df in severe_samples)
                if remaining > 0:
                    already_taken = pd.concat(severe_samples) if severe_samples else pd.DataFrame()
                    remaining_flights = severe_delay_sorted[~severe_delay_sorted.index.isin(already_taken.index)]
                    if len(remaining_flights) > 0:
                        additional = remaining_flights.head(remaining)
                        severe_samples.append(additional)
                
                if severe_samples:
                    severe_sampled = pd.concat(severe_samples, ignore_index=True)
                    sampled_month.append(severe_sampled)
                    print(f"L·∫•y {len(severe_sampled)} d√≤ng")
                else:
                    print(f"Kh√¥ng l·∫•y ƒë∆∞·ª£c")
            else:
                print(f"Kh√¥ng ƒë·ªß d·ªØ li·ªáu")
        else:
            print(f"Kh√¥ng c√≥ d·ªØ li·ªáu")
        
        # 5.2. L·∫•y m·∫´u TR·ªÑ V·ª™A (100 d√≤ng)
        print(f"     üüß Tr·ªÖ v·ª´a: ", end="")
        if len(moderate_delay) > 0:
            # ∆Øu ti√™n: gi·ªù cao ƒëi·ªÉm, s√¢n bay ƒë√¥ng, th·ªùi ti·∫øt x·∫•u
            moderate_delay['score'] = (
                moderate_delay['DEP_HOUR'].apply(lambda x: 2 if x in peak_hours else 0) +
                moderate_delay['ORIGIN'].apply(lambda x: 1.5 if x in top_airports else 0) +
                moderate_delay.apply(lambda row: 2 if (pd.notna(row.get('O_PRCP')) and row.get('O_PRCP', 0) > 0) else 0, axis=1)
            )
            
            moderate_delay_sorted = moderate_delay.sort_values(
                by=['score', 'ARR_DELAY'], 
                ascending=[False, False]
            )
            
            n_moderate = min(100, len(moderate_delay_sorted))
            if n_moderate > 0:
                # ƒê·∫£m b·∫£o ph√¢n b·ªë theo h√£ng
                moderate_samples = []
                carriers = moderate_delay_sorted['OP_CARRIER'].unique()
                
                for carrier in carriers[:8]:  # L·∫•y 8 h√£ng ƒë·∫ßu
                    carrier_flights = moderate_delay_sorted[moderate_delay_sorted['OP_CARRIER'] == carrier]
                    if len(carrier_flights) > 0:
                        n_carrier = min(15, len(carrier_flights))
                        moderate_samples.append(carrier_flights.head(n_carrier))
                
                moderate_sampled = pd.concat(moderate_samples, ignore_index=True)
                
                # N·∫øu ch∆∞a ƒë·ªß, l·∫•y th√™m ng·∫´u nhi√™n
                if len(moderate_sampled) < n_moderate:
                    remaining = moderate_delay_sorted[~moderate_delay_sorted.index.isin(moderate_sampled.index)]
                    if len(remaining) > 0:
                        additional = remaining.head(n_moderate - len(moderate_sampled))
                        moderate_sampled = pd.concat([moderate_sampled, additional], ignore_index=True)
                
                moderate_sampled = moderate_sampled.head(n_moderate)
                sampled_month.append(moderate_sampled)
                print(f"L·∫•y {len(moderate_sampled)} d√≤ng")
            else:
                print(f"Kh√¥ng ƒë·ªß d·ªØ li·ªáu")
        else:
            print(f"Kh√¥ng c√≥ d·ªØ li·ªáu")
        
        # 5.3. L·∫•y m·∫´u TR·ªÑ √çT (150 d√≤ng)
        print(f"     üü® Tr·ªÖ √≠t: ", end="")
        if len(minor_delay) > 0:
            # ƒê·∫°i di·ªán c√°c h√£ng, s√¢n bay, gi·ªù kh√°c nhau
            n_minor = min(150, len(minor_delay))
            
            if n_minor > 0:
                # Ph√¢n t·∫ßng theo h√£ng
                minor_samples = []
                carriers = minor_delay['OP_CARRIER'].unique()
                
                for carrier in carriers:
                    carrier_flights = minor_delay[minor_delay['OP_CARRIER'] == carrier]
                    if len(carrier_flights) > 0:
                        # T√≠nh t·ª∑ l·ªá d·ª±a tr√™n s·ªë l∆∞·ª£ng chuy·∫øn c·ªßa h√£ng
                        proportion = len(carrier_flights) / len(minor_delay)
                        n_from_carrier = max(2, int(n_minor * proportion))
                        n_from_carrier = min(n_from_carrier, len(carrier_flights))
                        
                        if n_from_carrier > 0:
                            sampled = carrier_flights.sample(n=n_from_carrier, random_state=random_seed)
                            minor_samples.append(sampled)
                
                if minor_samples:
                    minor_sampled = pd.concat(minor_samples, ignore_index=True)
                    
                    # N·∫øu ch∆∞a ƒë·ªß, l·∫•y th√™m ng·∫´u nhi√™n
                    if len(minor_sampled) < n_minor:
                        remaining = n_minor - len(minor_sampled)
                        all_minor = minor_delay[~minor_delay.index.isin(minor_sampled.index)]
                        if len(all_minor) > 0:
                            additional = all_minor.sample(n=min(remaining, len(all_minor)), random_state=random_seed)
                            minor_sampled = pd.concat([minor_sampled, additional], ignore_index=True)
                    
                    minor_sampled = minor_sampled.head(n_minor)
                    sampled_month.append(minor_sampled)
                    print(f"L·∫•y {len(minor_sampled)} d√≤ng")
                else:
                    print(f"Kh√¥ng l·∫•y ƒë∆∞·ª£c")
            else:
                print(f"Kh√¥ng ƒë·ªß d·ªØ li·ªáu")
        else:
            print(f"Kh√¥ng c√≥ d·ªØ li·ªáu")
        
        # 5.4. L·∫•y m·∫´u ƒê√öNG GI·ªú (200 d√≤ng)
        print(f"     üü© ƒê√∫ng gi·ªù: ", end="")
        if len(on_time) > 0:
            # ∆Øu ti√™n: gi·ªù cao ƒëi·ªÉm, th·ªùi ti·∫øt x·∫•u m√† v·∫´n ƒë√∫ng gi·ªù
            on_time['score'] = (
                on_time['DEP_HOUR'].apply(lambda x: 3 if x in peak_hours else 0) +
                on_time.apply(lambda row: 3 if (pd.notna(row.get('O_PRCP')) and row.get('O_PRCP', 0) > 0) else 0, axis=1) +
                on_time.apply(lambda row: 3 if (pd.notna(row.get('O_WSPD')) and row.get('O_WSPD', 0) > 20) else 0, axis=1)
            )
            
            on_time_sorted = on_time.sort_values(by=['score'], ascending=False)
            
            n_ontime = min(200, len(on_time_sorted))
            
            if n_ontime > 0:
                # L·∫•y c√°c chuy·∫øn ƒë·∫∑c bi·ªát tr∆∞·ªõc
                special_cases = on_time_sorted[on_time_sorted['score'] > 0]
                n_special = min(50, len(special_cases))
                
                if n_special > 0:
                    special_sampled = special_cases.head(n_special)
                else:
                    special_sampled = pd.DataFrame()
                
                # L·∫•y th√™m c√°c chuy·∫øn th√¥ng th∆∞·ªùng
                remaining_needed = n_ontime - len(special_sampled)
                if remaining_needed > 0:
                    regular_cases = on_time_sorted[on_time_sorted['score'] == 0]
                    if len(regular_cases) > 0:
                        regular_sampled = regular_cases.sample(
                            n=min(remaining_needed, len(regular_cases)), 
                            random_state=random_seed
                        )
                    else:
                        regular_sampled = pd.DataFrame()
                
                if not special_sampled.empty or not regular_sampled.empty:
                    ontime_sampled = pd.concat([special_sampled, regular_sampled], ignore_index=True)
                    ontime_sampled = ontime_sampled.head(n_ontime)
                    sampled_month.append(ontime_sampled)
                    print(f"L·∫•y {len(ontime_sampled)} d√≤ng")
                else:
                    print(f"Kh√¥ng l·∫•y ƒë∆∞·ª£c")
            else:
                print(f"Kh√¥ng ƒë·ªß d·ªØ li·ªáu")
        else:
            print(f"Kh√¥ng c√≥ d·ªØ li·ªáu")
        
        # 6. K·∫øt h·ª£p t·∫•t c·∫£ m·∫´u c·ªßa th√°ng
        if sampled_month:
            month_sampled = pd.concat(sampled_month, ignore_index=True)
            
            # Gi·ªõi h·∫°n s·ªë d√≤ng m·ªói th√°ng (~500)
            month_sampled = month_sampled.head(sample_per_month)
            
            # Th√™m c·ªôt th√°ng ƒë·ªÉ theo d√µi
            month_sampled['SAMPLING_MONTH'] = month
            
            all_sampled_data.append(month_sampled)
            
            print(f"\n   ‚úÖ Th√°ng {month:02d}: L·∫•y ƒë∆∞·ª£c {len(month_sampled)} d√≤ng")
            print(f"      Ph√¢n b·ªë: Tr·ªÖ nhi·ªÅu: {len(month_sampled[month_sampled['ARR_DELAY'] > 60])}, "
                  f"Tr·ªÖ v·ª´a: {len(month_sampled[(month_sampled['ARR_DELAY'] > 30) & (month_sampled['ARR_DELAY'] <= 120)])}, "
                  f"Tr·ªÖ √≠t: {len(month_sampled[(month_sampled['ARR_DELAY'] > 0) & (month_sampled['ARR_DELAY'] <= 30)])}, "
                  f"ƒê√∫ng gi·ªù: {len(month_sampled[month_sampled['ARR_DELAY'] <= 0])}")
        else:
            print(f"\n   ‚ùå Th√°ng {month:02d}: Kh√¥ng l·∫•y ƒë∆∞·ª£c d·ªØ li·ªáu n√†o")
    
    # 7. K·∫øt h·ª£p t·∫•t c·∫£ th√°ng
    print(f"\n{'=' * 70}")
    print("üì¶ K·∫æT H·ª¢P K·∫æT QU·∫¢")
    print(f"{'=' * 70}")
    
    if not all_sampled_data:
        print("‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu n√†o ƒë∆∞·ª£c l·∫•y m·∫´u!")
        return
    
    final_sampled = pd.concat(all_sampled_data, ignore_index=True)
    
    # X√≥a c·ªôt score n·∫øu c√≥
    if 'score' in final_sampled.columns:
        final_sampled = final_sampled.drop(columns=['score'])
    
    # X√≥a c·ªôt DEP_HOUR t·∫°m th·ªùi n·∫øu c√≥
    if 'DEP_HOUR' in final_sampled.columns:
        final_sampled = final_sampled.drop(columns=['DEP_HOUR'])
    
    print(f"üìä T·ªïng s·ªë d√≤ng sampling: {len(final_sampled):,}")
    
    # 8. Ph√¢n t√≠ch t√≠nh c√¢n b·∫±ng
    print(f"\nüìà PH√ÇN T√çCH T√çNH C√ÇN B·∫∞NG:")
    
    # 8.1. Ph√¢n b·ªë theo th√°ng
    print(f"\n   üìÖ Ph√¢n b·ªë theo th√°ng:")
    month_dist = final_sampled['SAMPLING_MONTH'].value_counts().sort_index()
    for month, count in month_dist.items():
        print(f"      - Th√°ng {month:02d}: {count:3d} d√≤ng")
    
    # 8.2. Ph√¢n b·ªë theo h√£ng bay
    print(f"\n   ‚úàÔ∏è Ph√¢n b·ªë theo h√£ng bay:")
    carrier_dist = final_sampled['OP_CARRIER'].value_counts()
    print(f"      T·ªïng s·ªë h√£ng: {len(carrier_dist)}")
    for carrier, count in carrier_dist.head(10).items():
        print(f"      - {carrier}: {count:3d} chuy·∫øn")
    
    # 8.3. Ph√¢n b·ªë theo s√¢n bay
    print(f"\n   üè¢ Ph√¢n b·ªë theo s√¢n bay (xu·∫•t ph√°t):")
    origin_dist = final_sampled['ORIGIN'].value_counts()
    print(f"      T·ªïng s·ªë s√¢n bay: {len(origin_dist)}")
    
    # Ki·ªÉm tra c√≥ bao nhi√™u s√¢n bay l·ªõn ƒë∆∞·ª£c bao ph·ªß
    covered_top_airports = [ap for ap in top_airports if ap in origin_dist.index]
    print(f"      S√¢n bay l·ªõn ƒë∆∞·ª£c bao ph·ªß: {len(covered_top_airports)}/{len(top_airports)}")
    
    # 8.4. Ph√¢n b·ªë theo ƒë·ªô tr·ªÖ
    print(f"\n   ‚è±Ô∏è Ph√¢n b·ªë theo ƒë·ªô tr·ªÖ:")
    delay_categories = {
        'Tr·ªÖ nhi·ªÅu (>60p)': len(final_sampled[final_sampled['ARR_DELAY'] > 60]),
        'Tr·ªÖ v·ª´a (30-120p)': len(final_sampled[(final_sampled['ARR_DELAY'] > 30) & (final_sampled['ARR_DELAY'] <= 120)]),
        'Tr·ªÖ √≠t (0-30p)': len(final_sampled[(final_sampled['ARR_DELAY'] > 0) & (final_sampled['ARR_DELAY'] <= 30)]),
        'ƒê√∫ng gi·ªù (‚â§0p)': len(final_sampled[final_sampled['ARR_DELAY'] <= 0])
    }
    
    for category, count in delay_categories.items():
        percentage = (count / len(final_sampled)) * 100
        print(f"      - {category}: {count:3d} d√≤ng ({percentage:.1f}%)")
    
    # 8.5. Ki·ªÉm tra c√°c tr∆∞·ªùng h·ª£p c√≥ th·ªÉ so s√°nh
    print(f"\n   üîÑ Ki·ªÉm tra kh·∫£ nƒÉng so s√°nh:")
    
    # ƒê·∫øm s·ªë c·∫∑p c√πng tuy·∫øn bay
    route_counts = final_sampled.groupby(['ORIGIN', 'DEST']).size()
    routes_with_multiple = (route_counts > 1).sum()
    print(f"      - S·ªë tuy·∫øn bay c√≥ >1 chuy·∫øn: {routes_with_multiple}")
    
    # ƒê·∫øm s·ªë c·∫∑p c√πng h√£ng c√πng gi·ªù
    if 'DEP_TIME' in final_sampled.columns:
        final_sampled['DEP_HOUR'] = final_sampled['DEP_TIME'].astype(str).str[:2].fillna('00').astype(int)
        hour_carrier_counts = final_sampled.groupby(['OP_CARRIER', 'DEP_HOUR']).size()
        hour_carrier_pairs = (hour_carrier_counts > 1).sum()
        print(f"      - S·ªë c·∫∑p h√£ng-gi·ªù c√≥ >1 chuy·∫øn: {hour_carrier_pairs}")
    
    # 9. L∆∞u file CSV
    print(f"\nüíæ ƒêang l∆∞u file CSV...")
    final_sampled.to_csv(output_csv, index=False, encoding='utf-8')
    
    file_size_mb = Path(output_csv).stat().st_size / (1024 ** 2)
    print(f"‚úÖ ƒê√£ l∆∞u: {output_csv}")
    print(f"üìè K√≠ch th∆∞·ªõc file: {file_size_mb:.2f} MB")
    print(f"üë• T·ªïng s·ªë d√≤ng: {len(final_sampled):,}")
    
    # 10. L∆∞u report
    report_path = output_path.parent / "sampling_report.txt"
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("=" * 70 + "\n")
        f.write("SAMPLING REPORT\n")
        f.write("=" * 70 + "\n\n")
        f.write(f"Sampling date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"Input directory: {parquet_dir}\n")
        f.write(f"Output file: {output_csv}\n")
        f.write(f"Total rows sampled: {len(final_sampled)}\n\n")
        
        f.write("MONTHLY DISTRIBUTION:\n")
        f.write("-" * 30 + "\n")
        for month, count in month_dist.items():
            f.write(f"Month {month:02d}: {count:3d} rows\n")
        
        f.write("\nCARRIER DISTRIBUTION (Top 15):\n")
        f.write("-" * 30 + "\n")
        for carrier, count in carrier_dist.head(15).items():
            f.write(f"{carrier}: {count:3d} flights\n")
        
        f.write("\nDELAY CATEGORY DISTRIBUTION:\n")
        f.write("-" * 30 + "\n")
        for category, count in delay_categories.items():
            percentage = (count / len(final_sampled)) * 100
            f.write(f"{category}: {count:3d} rows ({percentage:.1f}%)\n")
        
        f.write("\nBALANCE CHECK:\n")
        f.write("-" * 30 + "\n")
        f.write(f"Top carriers covered: {len(set(top_carriers) & set(carrier_dist.index))}/{len(top_carriers)}\n")
        f.write(f"Top airports covered: {len(covered_top_airports)}/{len(top_airports)}\n")
        f.write(f"Routes with comparisons: {routes_with_multiple}\n")
        
        if 'hour_carrier_pairs' in locals():
            f.write(f"Hour-carrier pairs: {hour_carrier_pairs}\n")
    
    print(f"üìù ƒê√£ l∆∞u b√°o c√°o: {report_path}")
    print(f"\nüéâ HO√ÄN TH√ÄNH STRATIFIED SAMPLING!")


# Main execution
if __name__ == "__main__":
    # C·∫•u h√¨nh
    PARQUET_DIR = "D:/UEL/DA_AVD/ADA-Flight-Delay/data/parquet/flights_weather"
    OUTPUT_CSV = "D:/UEL/DA_AVD/ADA-Flight-Delay/data/sampled/flight_data_sampled_2023.csv"
    
    # Ch·∫°y sampling
    stratified_sampling_from_parquet(
        parquet_dir=PARQUET_DIR,
        output_csv=OUTPUT_CSV,
        target_rows=6000,
        sample_per_month=500,
        random_seed=42
    )