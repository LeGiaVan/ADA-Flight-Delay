import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import numpy as np
import sys
import warnings

# T·∫Øt c·∫£nh b√°o kh√¥ng c·∫ßn thi·∫øt
warnings.simplefilter(action='ignore', category=FutureWarning)

def validate_sampling_quality(parquet_dir, sample_csv_path):
    """
    Ki·ªÉm ƒë·ªãnh t√≠nh ƒë·∫°i di·ªán (Phi√™n b·∫£n Robust - Ch·ªëng l·ªói Schema Mismatch).
    """
    
    # ---------------------------------------------------------
    # 0. THI·∫æT L·∫¨P
    # ---------------------------------------------------------
    sample_path = Path(sample_csv_path)
    report_dir = sample_path.parent.parent / 'report'
    report_dir.mkdir(parents=True, exist_ok=True)
    report_txt_path = report_dir / 'validation_report.txt'
    
    log_buffer = []

    def log(text=""):
        print(text)
        log_buffer.append(str(text))

    log(f"üöÄ B·∫ÆT ƒê·∫¶U KI·ªÇM ƒê·ªäNH (VALIDATION)...")
    log(f"üìÇ Report Directory: {report_dir}")
    log("-" * 60)

    # ---------------------------------------------------------
    # B∆Ø·ªöC 1: LOAD D·ªÆ LI·ªÜU
    # ---------------------------------------------------------
    
    # 1.1 ƒê·ªçc Sample
    log("1Ô∏è‚É£ ƒêang ƒë·ªçc d·ªØ li·ªáu M·∫´u (Sample)...")
    try:
        df_sample = pd.read_csv(sample_csv_path)
        log(f"   -> ƒê√£ load {len(df_sample):,} d√≤ng m·∫´u.")
    except Exception as e:
        log(f"‚ùå L·ªói ƒë·ªçc file m·∫´u: {e}")
        return

    # 1.2 ƒê·ªçc Population (FIX L·ªñI SCHEMA ·ªû ƒê√ÇY)
    log("2Ô∏è‚É£ ƒêang ƒë·ªçc d·ªØ li·ªáu T·ªïng th·ªÉ (Population)...")
    log("   -> Chuy·ªÉn sang ch·∫ø ƒë·ªô ƒë·ªçc t·ª´ng file (Iterative Read) ƒë·ªÉ tr√°nh l·ªói Schema...")
    
    try:
        # T√¨m t·∫•t c·∫£ file parquet
        files = list(Path(parquet_dir).rglob("*.parquet"))
        if not files:
            log("‚ùå Kh√¥ng t√¨m th·∫•y file parquet n√†o!")
            return
        
        log(f"   -> T√¨m th·∫•y {len(files)} file Parquet. ƒêang x·ª≠ l√Ω...")
        
        cols_needed = ['OP_CARRIER', 'FL_DATE', 'ORIGIN', 'DEP_TIME']
        dfs = []
        
        # Loop qua t·ª´ng file ƒë·ªÉ ƒë·ªçc
        for i, f in enumerate(files):
            try:
                # Ch·ªâ ƒë·ªçc c√°c c·ªôt c·∫ßn thi·∫øt, b·ªè qua c·ªôt 'month' g√¢y l·ªói
                df_part = pd.read_parquet(f, columns=cols_needed)
                dfs.append(df_part)
                
                # In ti·∫øn ƒë·ªô m·ªói 20 file
                if (i + 1) % 20 == 0:
                    print(f"      ... ƒê√£ ƒë·ªçc {i + 1}/{len(files)} file")
            except Exception as e:
                # N·∫øu file n√†o l·ªói qu√° th√¨ b·ªè qua, in warning nh·∫π
                continue
        
        if not dfs:
            log("‚ùå Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c file n√†o th√†nh c√¥ng.")
            return

        # G·ªôp l·∫°i th√†nh 1 DataFrame l·ªõn
        df_pop = pd.concat(dfs, ignore_index=True)
        
        # X·ª≠ l√Ω ng√†y th√°ng (T·ª± t√≠nh l·∫°i Month/Day ƒë·ªÉ ƒë·∫£m b·∫£o nh·∫•t qu√°n)
        if 'FL_DATE' in df_pop.columns:
            df_pop['FL_DATE'] = pd.to_datetime(df_pop['FL_DATE'])
            df_pop['MONTH'] = df_pop['FL_DATE'].dt.month
            df_pop['DAY_OF_WEEK'] = df_pop['FL_DATE'].dt.dayofweek + 1
        
        # X·ª≠ l√Ω gi·ªù bay
        def extract_hour(df):
            if 'DEP_TIME' in df.columns:
                return pd.to_numeric(df['DEP_TIME'], errors='coerce').fillna(0).astype(int) // 100
            return None

        df_pop['DEP_HOUR'] = extract_hour(df_pop)
        df_sample['DEP_HOUR'] = extract_hour(df_sample)
            
        log(f"   -> ‚úÖ ƒê√£ load th√†nh c√¥ng {len(df_pop):,} d√≤ng t·ªïng th·ªÉ.")
        
    except Exception as e:
        log(f"‚ùå L·ªói nghi√™m tr·ªçng khi ƒë·ªçc Population: {e}")
        import traceback
        traceback.print_exc()
        return

    # ---------------------------------------------------------
    # B∆Ø·ªöC 2: V·∫º V√Ä L∆ØU BI·ªÇU ƒê·ªí
    # ---------------------------------------------------------
    log("\nüé® ƒêang v·∫Ω v√† l∆∞u bi·ªÉu ƒë·ªì...")

    try:
        # --- Chart 1: Carrier ---
        pop_carrier = df_pop['OP_CARRIER'].value_counts(normalize=True).reset_index()
        pop_carrier.columns = ['Carrier', 'Percentage']
        pop_carrier['Type'] = 'Population'
        
        sample_carrier = df_sample['OP_CARRIER'].value_counts(normalize=True).reset_index()
        sample_carrier.columns = ['Carrier', 'Percentage']
        sample_carrier['Type'] = 'Sample'
        
        comp_carrier = pd.concat([pop_carrier, sample_carrier])
        
        plt.figure(figsize=(12, 6))
        sns.barplot(data=comp_carrier, x='Carrier', y='Percentage', hue='Type', palette=['#bdc3c7', '#e74c3c'])
        plt.title('Validation 1: Carrier Distribution', fontsize=14, fontweight='bold')
        plt.savefig(report_dir / 'val_1_carrier.png', bbox_inches='tight')
        plt.close()

        # --- Chart 2: Airport ---
        top_20_airports = df_pop['ORIGIN'].value_counts().head(20).index.tolist()
        pop_airport = df_pop[df_pop['ORIGIN'].isin(top_20_airports)]['ORIGIN'].value_counts(normalize=True).reset_index()
        pop_airport.columns = ['Airport', 'Percentage']
        pop_airport['Type'] = 'Population'
        sample_airport = df_sample[df_sample['ORIGIN'].isin(top_20_airports)]['ORIGIN'].value_counts(normalize=True).reset_index()
        sample_airport.columns = ['Airport', 'Percentage']
        sample_airport['Type'] = 'Sample'
        comp_airport = pd.concat([pop_airport, sample_airport])
        
        plt.figure(figsize=(14, 6))
        sns.barplot(data=comp_airport, x='Airport', y='Percentage', hue='Type', palette=['#bdc3c7', '#3498db'])
        plt.title('Validation 2: Top 20 Origin Airports Distribution', fontsize=14, fontweight='bold')
        plt.xticks(rotation=45)
        plt.savefig(report_dir / 'val_2_airport.png', bbox_inches='tight')
        plt.close()

        # --- Chart 3: Peak Hour ---
        pop_hour = df_pop['DEP_HOUR'].value_counts(normalize=True).sort_index().reset_index()
        pop_hour.columns = ['Hour', 'Percentage']
        pop_hour['Type'] = 'Population'
        sample_hour = df_sample['DEP_HOUR'].value_counts(normalize=True).sort_index().reset_index()
        sample_hour.columns = ['Hour', 'Percentage']
        sample_hour['Type'] = 'Sample'
        comp_hour = pd.concat([pop_hour, sample_hour])
        
        plt.figure(figsize=(12, 6))
        sns.lineplot(data=comp_hour, x='Hour', y='Percentage', hue='Type', style='Type', markers=True, dashes=False, palette=['gray', 'red'])
        plt.axvspan(6, 9, color='yellow', alpha=0.2, label='Peak Morning')
        plt.axvspan(16, 19, color='orange', alpha=0.2, label='Peak Afternoon')
        plt.title('Validation 3: Departure Hour Distribution', fontsize=14, fontweight='bold')
        plt.xticks(range(0, 24))
        plt.grid(True, linestyle='--', alpha=0.7)
        plt.savefig(report_dir / 'val_3_peak_hour.png', bbox_inches='tight')
        plt.close()

        log("   ‚úÖ ƒê√£ l∆∞u 3 bi·ªÉu ƒë·ªì th√†nh c√¥ng.")

    except Exception as e:
        log(f"‚ùå L·ªói khi v·∫Ω bi·ªÉu ƒë·ªì: {e}")

    # ---------------------------------------------------------
    # B∆Ø·ªöC 3: GHI REPORT
    # ---------------------------------------------------------
    log("\nüìã K·∫æT QU·∫¢ KI·ªÇM ƒê·ªäNH CHI TI·∫æT")
    log("=" * 60)
    
    # Temporal
    missing_months = set(range(1, 13)) - set(df_sample['MONTH'].unique())
    missing_days = set(range(1, 8)) - set(df_sample['DAY_OF_WEEK'].unique())
    log(f"1. TEMPORAL COVERAGE:")
    log(f"   - Months: {'‚úÖ ƒê·ªß 12 th√°ng' if not missing_months else f'‚ùå Thi·∫øu: {missing_months}'}")
    log(f"   - Days:   {'‚úÖ ƒê·ªß 7 ng√†y' if not missing_days else f'‚ùå Thi·∫øu: {missing_days}'}")

    # Peak Hour
    peak_hours = list(range(6, 10)) + list(range(16, 20))
    pop_peak_ratio = df_pop['DEP_HOUR'].isin(peak_hours).mean()
    sample_peak_ratio = df_sample['DEP_HOUR'].isin(peak_hours).mean()
    diff_peak = abs(pop_peak_ratio - sample_peak_ratio)
    log(f"\n2. PEAK HOUR (6-9h, 16-19h):")
    log(f"   - Pop: {pop_peak_ratio:.2%} | Sample: {sample_peak_ratio:.2%} | Diff: {diff_peak:.2%}")

    # Carriers
    log(f"\n3. CARRIER CONSISTENCY (Top 5):")
    merged = pd.merge(pop_carrier, sample_carrier, on='Carrier', suffixes=('_Pop', '_Sample'))
    merged['Diff'] = abs(merged['Percentage_Pop'] - merged['Percentage_Sample'])
    
    log(f"   {'Carrier':<8} | {'Pop %':<10} | {'Sample %':<10} | {'Diff %':<10}")
    log(f"   {'-'*45}")
    for _, row in merged.head(5).iterrows():
        log(f"   {row['Carrier']:<8} | {row['Percentage_Pop']:.2%}     | {row['Percentage_Sample']:.2%}     | {row['Diff']:.2%}")

    # Save TXT
    try:
        with open(report_txt_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(log_buffer))
        log(f"\nüíæ ƒê√£ l∆∞u b√°o c√°o t·∫°i: {report_txt_path}")
    except Exception as e:
        print(f"‚ùå L·ªói l∆∞u file txt: {e}")

# --- CH·∫†Y CODE ---
if __name__ == "__main__":
    # ƒê∆Ø·ªúNG D·∫™N C·ª¶A B·∫†N
    PARQUET_DIR = "D:/UEL/ADA/ADA-Flight-Delay/data/parquet/flights_weather/year=2023"
    SAMPLE_CSV = "D:/UEL/ADA/ADA-Flight-Delay/data/sampled/flight_data_sampled_2023.csv"
    
    validate_sampling_quality(PARQUET_DIR, SAMPLE_CSV)